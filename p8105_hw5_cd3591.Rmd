---
title: "p8105_hw5_cd3591"
author: "Anny"
date: "2025-11-07"
output: github_document
---

```{r}
# set up libraries
library("tidyverse")
```

# Problem 1
```{r}
# Write a function to check whether there are duplicate birthdays in the group; and return TRUE or FALSE based on the result.
birthday_shared = function(n_people){
  birthdays = sample(1:365, n_people, replace = TRUE)
  any(duplicated(birthdays))
}

# simulate 10000 times and calculate the prob
set.seed(1122)
birthday_sim_results =
  tibble(n_people = 2:50) %>% # for each group size between 2 and 50
  mutate(
    prob_shared = map_dbl(n_people,
      ~ mean(replicate(10000, birthday_shared(.x))) # run this function 10000 times & averaging across the 10000 simulation runs
    )
  )

birthday_sim_results

# make a plot
birthday_sim_results %>%
  ggplot(aes(x = n_people, y = prob_shared)) +
  geom_line() +
  geom_point() +
  labs(
    x = "Group size",
    y = "Estimated P(at least one shared birthday)",
    title = "Birthday problem simulation (10,000 runs per n_people)"
  ) +
  theme_minimal()

```

**Comments**: The plot and table show that the probability of at least one shared birthday is essentially zero for very small groups, but increases rapidly as `n_people` increases. In specific, when `n_people` = ~23, the estimated probability is about 0.50, meaning a shared birthday is already more likely than not. By `n_people` = ~35, the probability exceeds 0.80, and by `n_people` = ~45 it is above 0.94; at `n_people` = ~50, the probability is roughly 0.97. This indicates that even though there are 365 possible birthdays, only a relatively small group is needed for a high likelihood of shared birthdays.

# Problem 2
```{r}
set.seed(8105)

# function to simulate 1-sample t-tests for a given true mean mu
simulate_t_tests <- function(mu, n = 30, sigma = 5, n_sim = 5000) {
  tibble(sim = 1:n_sim) %>%
    mutate(
      x = map(sim, ~ rnorm(n, mean = mu, sd = sigma)),
      t_stats   = map(x, ~ t.test(.x, mu = 0)),
      tidy_output = map(t_stats, broom::tidy)
    ) %>%
    unnest(tidy_output) %>%
    transmute(
      mu_true = mu,
      sim,
      estimate,              
      p_value = p.value
    )
}

# Repeat the above for μ={1,2,3,4,5,6}
results_list = lapply(0:6, simulate_t_tests)   
sim_results = bind_rows(results_list)   

# the power of the test vs true mean
power_results =
  sim_results %>%
  group_by(mu_true) %>%
  summarize(
    power = mean(p_value < 0.05),
    .groups = "drop"
  )

power_results

# make a plot
power_results %>%
  ggplot(aes(x = mu_true, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True mean (μ)",
    y = "Power",
    title = "Power of one-sample t-test (n = 30, σ = 5, α = 0.05)"
  ) +
  theme_minimal()
```

**Comments**: Power increases as the effect size (the true mean μ moving away from 0) increases. In specific, when the true mean = 0, the rejection rate is about 0.05 (equals the type I error). As the true mean grows to 1, 2, and 3, power rises to roughly 0.20, 0.55, and 0.88, and by true mean ≥ 4, the test almost always rejects the null. 

```{r}
# make a table for average estimate for all samples and mean estimate only in samples for which the null was rejected 
mean_results =
  sim_results %>%
  group_by(mu_true) %>%
  summarize(
    mean_estimate = mean(estimate),
    mean_estimate_reject = mean(estimate[p_value < 0.05]),
    .groups = "drop"
  )

mean_results

# plot 1: average estimate using ALL samples
ggplot(mean_results, aes(x = mu_true, y = mean_estimate)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True mean (μ)",
    y = "Average estimate (all samples)",
    title = "Average estimate vs. true mean (all tests)"
  ) +
  theme_minimal()

# plot 2: average estimate using ONLY significant results
ggplot(mean_results, aes(x = mu_true, y = mean_estimate_reject)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True mean (μ)",
    y = "Average estimate (p < 0.05 only)",
    title = "Average estimate vs. true mean (significant tests only)"
  ) +
  theme_minimal()
```
**Answer**: No. the sample average of μ_hat among tests where the null is rejected is not approximately equal to the true μ. Based on the graphs above, in the first plot (all tests), the points form an almost straight line on top of the 45° line: the average estimate closely matches the true mean μ, so μ_hat is approximately unbiased overall. However, in the second plot (significant tests only), the points lie clearly above the 45° line for smaller and moderate values of μ; for example, when μ=1 or 2, the average estimate among rejected tests is much larger than the true value. This happens because we only keep samples with p<0.05, i.e., those where μ_hat is far from 0 in the direction of the true effect. 

